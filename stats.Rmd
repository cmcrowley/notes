---
title: "Technical notes"
author: Cynthia Crowley
date: "`r Sys.Date()`"
output: html_document
params:
  questions: "Q:"
extra_dependencies:
  amsmath: null
---
# On probability

- Conditional probability

\begin{equation}
  P(A|B) = \frac{P(A \cap B)}{P(B)} 
\end{equation}

  - Independence
  If the conditional probability of A|B equals the unconditional probability of A then A is independent of B. 
$$ P(A|B)=P(A) \implies P(A\cap B) = P(A)P(B) $$
  Note that the second equation results from multiplying both sides of \ref{cp}. The probabilities of combinations of independent events are multiplicative.
  
- Unconditional probability
$$ P(A)=P(A|B) + P(A | not B) $$

#### Deriving Bayes Rule
Most usage: to go from $P(D|H)$ (the likelihood of the data assuming the hypothesis is true) to what we want to know $P(H|D)$
$$P(H|D) = \frac{P(D\cap H)}{P(D)} = \frac{P(D|H)P(H)}{P(D)}$$
The probability of the hypothesis given the data is equal to the probability of the data given the hypothesis (the likelihood) times the probability of the hypothesis divided by the probability of the data. Raises two problems:

  1. What is $P|H$?
    - The unconditional, or prior, probability of the hypothesis $P(H_i)$
  2. What is the unconditional probability of the data, $P(D)$?
    - This is caclulatable if we have an exhaustive set of mutually exclusive hypotheses.
  
Bayes' Rule for a particular hypothesis $H_i$ when it is one of a mutually exclusive set of hypotheses ${H_j}$. 

$$ P(H_i|D) = \frac{P(D|H_i)H(H_i)}{\sum_{j}{P(H_j)P(D|H_J)}} $$


- Base rate fallacy: Assuming that a positive test is significant. ($P(+|I) != P(I|+)$)

Probability of false positives for rare diseases
One person in 100 who doesn't have the disease will test positive anyway $\equiv P(+|U) = 1/100$, but if the population probability of being infected is $P(I)=10^{-6}$, then the probability $P(I|+) ~ 10^{-2}$. 

# On randomness, probability distributions
### EMD Chapter 4

- "Unfortunately, classical nonparametric approaches make it much harder to draw quantitative conclusions from data (rather than simply rejecting or failing to reject null hypotheses about differences between groups")
- Stochastic models don't just treat variability as a nuisance, but actually tells us something about processes. For ex., census counts that follow a negative binomial distribution tell us there is some form of environmental variation or aggregative response among individuals that we haven't taken into account. (Shaw and Dobson, 1995)
- Variability feeds back to change the mean behavior of ecological systems
  - Damselfish system (ch. 2), the number of recruits in any given cohort is the number of settlers surviving density-dependent mortality, but the average number of recruits is lower than expected from an average-sized cohort of settlers because large cohorts suffer disproportionately high mortality and contribute relatively little to the average. 
  -   Q: then where does the 'expected' value of recruits come from?
    - follows from Jensen's inequality (ruel and Ayres, 1999; Inouye, 2005)
    
#### Deriving a zero-inflated binomial distribution (using simple probability rules)



# Questions that pop up
- Is a Poisson-modeled variable really a binomial-process variable with less knowledge (number of events without knowledge of number of trials).

# Citations
- Ecological Models and Data in R by Ben Bolker
